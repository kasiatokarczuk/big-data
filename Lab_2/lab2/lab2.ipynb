{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82854fa4-2602-495f-b1d3-fa23e93629b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Zadanie 2\n",
    "Wybierz jeden z plików csv z poprzednich ćwiczeń (Mini Kurs, Actors.csv, Names.csv ect..) lub inny plik z dbfs:/databricks-datasets/ i na jego podstawie stwórz schemat danych. Przykład na wykładzie. \n",
    "\n",
    "a.\tUżyj funckji spark.read. i stwórz DataFrame (DataFrameReader) wczytując plik z użyciem schematu, który stworzyłeś.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ec4d005-fb16-422b-957c-9f8e9bb1c463",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "863adee0-b581-4686-8eb9-8746cfb06688",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/databricks-datasets/COVID/</td><td>COVID/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/README.md</td><td>README.md</td><td>976</td><td>1532468253000</td></tr><tr><td>dbfs:/databricks-datasets/Rdatasets/</td><td>Rdatasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/SPARK_README.md</td><td>SPARK_README.md</td><td>3359</td><td>1455043490000</td></tr><tr><td>dbfs:/databricks-datasets/adult/</td><td>adult/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/airlines/</td><td>airlines/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/amazon/</td><td>amazon/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/asa/</td><td>asa/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/atlas_higgs/</td><td>atlas_higgs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/bikeSharing/</td><td>bikeSharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cctvVideos/</td><td>cctvVideos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/credit-card-fraud/</td><td>credit-card-fraud/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs100/</td><td>cs100/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs110x/</td><td>cs110x/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/cs190/</td><td>cs190/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/data.gov/</td><td>data.gov/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/definitive-guide/</td><td>definitive-guide/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/delta-sharing/</td><td>delta-sharing/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flights/</td><td>flights/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flower_photos/</td><td>flower_photos/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/flowers/</td><td>flowers/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/genomics/</td><td>genomics/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/hail/</td><td>hail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/identifying-campaign-effectiveness/</td><td>identifying-campaign-effectiveness/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot/</td><td>iot/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/iot-stream/</td><td>iot-stream/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark/</td><td>learning-spark/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/learning-spark-v2/</td><td>learning-spark-v2/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/lending-club-loan-stats/</td><td>lending-club-loan-stats/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/med-images/</td><td>med-images/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/media/</td><td>media/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/mnist-digits/</td><td>mnist-digits/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/news20.binary/</td><td>news20.binary/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi/</td><td>nyctaxi/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/nyctaxi-with-zipcodes/</td><td>nyctaxi-with-zipcodes/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/online_retail/</td><td>online_retail/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/overlap-join/</td><td>overlap-join/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/power-plant/</td><td>power-plant/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/retail-org/</td><td>retail-org/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/rwe/</td><td>rwe/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sai-summit-2019-sf/</td><td>sai-summit-2019-sf/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sample_logs/</td><td>sample_logs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/samples/</td><td>samples/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sfo_customer_survey/</td><td>sfo_customer_survey/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/sms_spam_collection/</td><td>sms_spam_collection/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/songs/</td><td>songs/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/structured-streaming/</td><td>structured-streaming/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/timeseries/</td><td>timeseries/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/tpch/</td><td>tpch/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/travel_recommendations_realtime/</td><td>travel_recommendations_realtime/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/warmup/</td><td>warmup/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/weather/</td><td>weather/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wiki/</td><td>wiki/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wikipedia-datasets/</td><td>wikipedia-datasets/</td><td>0</td><td>0</td></tr><tr><td>dbfs:/databricks-datasets/wine-quality/</td><td>wine-quality/</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/databricks-datasets/COVID/",
         "COVID/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/README.md",
         "README.md",
         976,
         1532468253000
        ],
        [
         "dbfs:/databricks-datasets/Rdatasets/",
         "Rdatasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/SPARK_README.md",
         "SPARK_README.md",
         3359,
         1455043490000
        ],
        [
         "dbfs:/databricks-datasets/adult/",
         "adult/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/airlines/",
         "airlines/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/amazon/",
         "amazon/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/asa/",
         "asa/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/atlas_higgs/",
         "atlas_higgs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/bikeSharing/",
         "bikeSharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cctvVideos/",
         "cctvVideos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/credit-card-fraud/",
         "credit-card-fraud/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs100/",
         "cs100/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs110x/",
         "cs110x/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/cs190/",
         "cs190/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/data.gov/",
         "data.gov/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/definitive-guide/",
         "definitive-guide/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/delta-sharing/",
         "delta-sharing/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flights/",
         "flights/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flower_photos/",
         "flower_photos/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/flowers/",
         "flowers/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/genomics/",
         "genomics/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/hail/",
         "hail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/identifying-campaign-effectiveness/",
         "identifying-campaign-effectiveness/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot/",
         "iot/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/iot-stream/",
         "iot-stream/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark/",
         "learning-spark/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/learning-spark-v2/",
         "learning-spark-v2/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/lending-club-loan-stats/",
         "lending-club-loan-stats/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/med-images/",
         "med-images/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/media/",
         "media/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/mnist-digits/",
         "mnist-digits/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/news20.binary/",
         "news20.binary/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi/",
         "nyctaxi/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/nyctaxi-with-zipcodes/",
         "nyctaxi-with-zipcodes/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/online_retail/",
         "online_retail/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/overlap-join/",
         "overlap-join/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/power-plant/",
         "power-plant/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/retail-org/",
         "retail-org/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/rwe/",
         "rwe/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sai-summit-2019-sf/",
         "sai-summit-2019-sf/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sample_logs/",
         "sample_logs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/samples/",
         "samples/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sfo_customer_survey/",
         "sfo_customer_survey/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/sms_spam_collection/",
         "sms_spam_collection/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/songs/",
         "songs/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/structured-streaming/",
         "structured-streaming/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/timeseries/",
         "timeseries/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/tpch/",
         "tpch/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/travel_recommendations_realtime/",
         "travel_recommendations_realtime/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/warmup/",
         "warmup/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/weather/",
         "weather/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wiki/",
         "wiki/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wikipedia-datasets/",
         "wikipedia-datasets/",
         0,
         0
        ],
        [
         "dbfs:/databricks-datasets/wine-quality/",
         "wine-quality/",
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "isDbfsCommandResult": false
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%fs ls dbfs:/databricks-datasets/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5edbeb5-2e73-4878-acf2-d980c8b5dc94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>region</th><th>pop</th><th>SATV</th><th>SATM</th><th>percent</th><th>dollars</th><th>pay</th></tr></thead><tbody><tr><td>AL</td><td>ESC</td><td>4041</td><td>470</td><td>514</td><td>8</td><td>3.648</td><td>27</td></tr><tr><td>AK</td><td>PAC</td><td>550</td><td>438</td><td>476</td><td>42</td><td>7.887</td><td>43</td></tr><tr><td>AZ</td><td>MTN</td><td>3665</td><td>445</td><td>497</td><td>25</td><td>4.231</td><td>30</td></tr><tr><td>AR</td><td>WSC</td><td>2351</td><td>470</td><td>511</td><td>6</td><td>3.334</td><td>23</td></tr><tr><td>CA</td><td>PAC</td><td>29760</td><td>419</td><td>484</td><td>45</td><td>4.826</td><td>39</td></tr><tr><td>CO</td><td>MTN</td><td>3294</td><td>456</td><td>513</td><td>28</td><td>4.809</td><td>31</td></tr><tr><td>CN</td><td>NE</td><td>3287</td><td>430</td><td>471</td><td>74</td><td>7.914</td><td>43</td></tr><tr><td>DE</td><td>SA</td><td>666</td><td>433</td><td>470</td><td>58</td><td>6.016</td><td>35</td></tr><tr><td>DC</td><td>SA</td><td>607</td><td>409</td><td>441</td><td>68</td><td>8.21</td><td>39</td></tr><tr><td>FL</td><td>SA</td><td>12938</td><td>418</td><td>466</td><td>44</td><td>5.154</td><td>30</td></tr><tr><td>GA</td><td>SA</td><td>6478</td><td>401</td><td>443</td><td>57</td><td>4.86</td><td>29</td></tr><tr><td>HI</td><td>PAC</td><td>1108</td><td>404</td><td>481</td><td>52</td><td>5.008</td><td>32</td></tr><tr><td>ID</td><td>MTN</td><td>1007</td><td>466</td><td>502</td><td>17</td><td>3.2</td><td>25</td></tr><tr><td>IL</td><td>ENC</td><td>11431</td><td>466</td><td>528</td><td>16</td><td>5.062</td><td>34</td></tr><tr><td>IN</td><td>ENC</td><td>5544</td><td>408</td><td>459</td><td>54</td><td>5.051</td><td>32</td></tr><tr><td>IA</td><td>WNC</td><td>2777</td><td>511</td><td>577</td><td>5</td><td>4.839</td><td>28</td></tr><tr><td>KS</td><td>WNC</td><td>2478</td><td>492</td><td>548</td><td>10</td><td>5.009</td><td>29</td></tr><tr><td>KY</td><td>ESC</td><td>3685</td><td>473</td><td>521</td><td>10</td><td>4.39</td><td>29</td></tr><tr><td>LA</td><td>WSC</td><td>4220</td><td>476</td><td>517</td><td>9</td><td>4.012</td><td>26</td></tr><tr><td>ME</td><td>NE</td><td>1228</td><td>423</td><td>463</td><td>60</td><td>5.894</td><td>28</td></tr><tr><td>MD</td><td>SA</td><td>4781</td><td>430</td><td>478</td><td>59</td><td>6.184</td><td>38</td></tr><tr><td>MA</td><td>NE</td><td>6016</td><td>427</td><td>473</td><td>72</td><td>6.351</td><td>36</td></tr><tr><td>MI</td><td>ENC</td><td>9295</td><td>454</td><td>514</td><td>12</td><td>5.257</td><td>38</td></tr><tr><td>MN</td><td>WNC</td><td>4375</td><td>477</td><td>542</td><td>14</td><td>5.26</td><td>33</td></tr><tr><td>MS</td><td>ESC</td><td>2573</td><td>477</td><td>519</td><td>4</td><td>3.322</td><td>24</td></tr><tr><td>MO</td><td>WNC</td><td>5117</td><td>473</td><td>522</td><td>12</td><td>4.415</td><td>28</td></tr><tr><td>MT</td><td>MTN</td><td>799</td><td>464</td><td>523</td><td>20</td><td>5.184</td><td>26</td></tr><tr><td>NE</td><td>WNC</td><td>1578</td><td>484</td><td>546</td><td>10</td><td>4.381</td><td>26</td></tr><tr><td>NV</td><td>MTN</td><td>1202</td><td>434</td><td>487</td><td>24</td><td>4.564</td><td>32</td></tr><tr><td>NH</td><td>NE</td><td>1109</td><td>442</td><td>486</td><td>67</td><td>5.504</td><td>31</td></tr><tr><td>NJ</td><td>MA</td><td>7730</td><td>418</td><td>473</td><td>69</td><td>9.159</td><td>38</td></tr><tr><td>NM</td><td>MTN</td><td>1515</td><td>480</td><td>527</td><td>12</td><td>4.446</td><td>26</td></tr><tr><td>NY</td><td>MA</td><td>17990</td><td>412</td><td>470</td><td>70</td><td>8.5</td><td>42</td></tr><tr><td>NC</td><td>SA</td><td>6629</td><td>401</td><td>440</td><td>55</td><td>4.802</td><td>29</td></tr><tr><td>ND</td><td>WNC</td><td>639</td><td>505</td><td>564</td><td>6</td><td>3.685</td><td>23</td></tr><tr><td>OH</td><td>ENC</td><td>10847</td><td>450</td><td>499</td><td>22</td><td>5.639</td><td>32</td></tr><tr><td>OK</td><td>WSC</td><td>3146</td><td>478</td><td>523</td><td>9</td><td>3.742</td><td>24</td></tr><tr><td>OR</td><td>PAC</td><td>2842</td><td>439</td><td>484</td><td>49</td><td>5.291</td><td>32</td></tr><tr><td>PA</td><td>MA</td><td>11882</td><td>420</td><td>463</td><td>64</td><td>6.534</td><td>36</td></tr><tr><td>RI</td><td>NE</td><td>1003</td><td>422</td><td>461</td><td>62</td><td>6.989</td><td>37</td></tr><tr><td>SC</td><td>SA</td><td>3487</td><td>397</td><td>437</td><td>54</td><td>4.327</td><td>28</td></tr><tr><td>SD</td><td>WNC</td><td>696</td><td>506</td><td>555</td><td>5</td><td>3.73</td><td>22</td></tr><tr><td>TN</td><td>ESC</td><td>4877</td><td>483</td><td>525</td><td>12</td><td>3.707</td><td>28</td></tr><tr><td>TX</td><td>WSC</td><td>16987</td><td>413</td><td>461</td><td>42</td><td>4.238</td><td>28</td></tr><tr><td>UT</td><td>MTN</td><td>1723</td><td>492</td><td>539</td><td>5</td><td>2.993</td><td>25</td></tr><tr><td>VT</td><td>NE</td><td>563</td><td>431</td><td>466</td><td>62</td><td>5.74</td><td>31</td></tr><tr><td>VA</td><td>SA</td><td>6187</td><td>425</td><td>470</td><td>58</td><td>5.36</td><td>32</td></tr><tr><td>WA</td><td>PAC</td><td>4867</td><td>437</td><td>486</td><td>44</td><td>5.045</td><td>33</td></tr><tr><td>WV</td><td>SA</td><td>1793</td><td>443</td><td>490</td><td>15</td><td>5.046</td><td>26</td></tr><tr><td>WI</td><td>ENC</td><td>4892</td><td>476</td><td>543</td><td>11</td><td>5.946</td><td>33</td></tr><tr><td>WY</td><td>MTN</td><td>454</td><td>458</td><td>519</td><td>13</td><td>5.255</td><td>29</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "AL",
         "ESC",
         4041,
         470,
         514,
         8,
         3.648,
         27
        ],
        [
         "AK",
         "PAC",
         550,
         438,
         476,
         42,
         7.887,
         43
        ],
        [
         "AZ",
         "MTN",
         3665,
         445,
         497,
         25,
         4.231,
         30
        ],
        [
         "AR",
         "WSC",
         2351,
         470,
         511,
         6,
         3.334,
         23
        ],
        [
         "CA",
         "PAC",
         29760,
         419,
         484,
         45,
         4.826,
         39
        ],
        [
         "CO",
         "MTN",
         3294,
         456,
         513,
         28,
         4.809,
         31
        ],
        [
         "CN",
         "NE",
         3287,
         430,
         471,
         74,
         7.914,
         43
        ],
        [
         "DE",
         "SA",
         666,
         433,
         470,
         58,
         6.016,
         35
        ],
        [
         "DC",
         "SA",
         607,
         409,
         441,
         68,
         8.21,
         39
        ],
        [
         "FL",
         "SA",
         12938,
         418,
         466,
         44,
         5.154,
         30
        ],
        [
         "GA",
         "SA",
         6478,
         401,
         443,
         57,
         4.86,
         29
        ],
        [
         "HI",
         "PAC",
         1108,
         404,
         481,
         52,
         5.008,
         32
        ],
        [
         "ID",
         "MTN",
         1007,
         466,
         502,
         17,
         3.2,
         25
        ],
        [
         "IL",
         "ENC",
         11431,
         466,
         528,
         16,
         5.062,
         34
        ],
        [
         "IN",
         "ENC",
         5544,
         408,
         459,
         54,
         5.051,
         32
        ],
        [
         "IA",
         "WNC",
         2777,
         511,
         577,
         5,
         4.839,
         28
        ],
        [
         "KS",
         "WNC",
         2478,
         492,
         548,
         10,
         5.009,
         29
        ],
        [
         "KY",
         "ESC",
         3685,
         473,
         521,
         10,
         4.39,
         29
        ],
        [
         "LA",
         "WSC",
         4220,
         476,
         517,
         9,
         4.012,
         26
        ],
        [
         "ME",
         "NE",
         1228,
         423,
         463,
         60,
         5.894,
         28
        ],
        [
         "MD",
         "SA",
         4781,
         430,
         478,
         59,
         6.184,
         38
        ],
        [
         "MA",
         "NE",
         6016,
         427,
         473,
         72,
         6.351,
         36
        ],
        [
         "MI",
         "ENC",
         9295,
         454,
         514,
         12,
         5.257,
         38
        ],
        [
         "MN",
         "WNC",
         4375,
         477,
         542,
         14,
         5.26,
         33
        ],
        [
         "MS",
         "ESC",
         2573,
         477,
         519,
         4,
         3.322,
         24
        ],
        [
         "MO",
         "WNC",
         5117,
         473,
         522,
         12,
         4.415,
         28
        ],
        [
         "MT",
         "MTN",
         799,
         464,
         523,
         20,
         5.184,
         26
        ],
        [
         "NE",
         "WNC",
         1578,
         484,
         546,
         10,
         4.381,
         26
        ],
        [
         "NV",
         "MTN",
         1202,
         434,
         487,
         24,
         4.564,
         32
        ],
        [
         "NH",
         "NE",
         1109,
         442,
         486,
         67,
         5.504,
         31
        ],
        [
         "NJ",
         "MA",
         7730,
         418,
         473,
         69,
         9.159,
         38
        ],
        [
         "NM",
         "MTN",
         1515,
         480,
         527,
         12,
         4.446,
         26
        ],
        [
         "NY",
         "MA",
         17990,
         412,
         470,
         70,
         8.5,
         42
        ],
        [
         "NC",
         "SA",
         6629,
         401,
         440,
         55,
         4.802,
         29
        ],
        [
         "ND",
         "WNC",
         639,
         505,
         564,
         6,
         3.685,
         23
        ],
        [
         "OH",
         "ENC",
         10847,
         450,
         499,
         22,
         5.639,
         32
        ],
        [
         "OK",
         "WSC",
         3146,
         478,
         523,
         9,
         3.742,
         24
        ],
        [
         "OR",
         "PAC",
         2842,
         439,
         484,
         49,
         5.291,
         32
        ],
        [
         "PA",
         "MA",
         11882,
         420,
         463,
         64,
         6.534,
         36
        ],
        [
         "RI",
         "NE",
         1003,
         422,
         461,
         62,
         6.989,
         37
        ],
        [
         "SC",
         "SA",
         3487,
         397,
         437,
         54,
         4.327,
         28
        ],
        [
         "SD",
         "WNC",
         696,
         506,
         555,
         5,
         3.73,
         22
        ],
        [
         "TN",
         "ESC",
         4877,
         483,
         525,
         12,
         3.707,
         28
        ],
        [
         "TX",
         "WSC",
         16987,
         413,
         461,
         42,
         4.238,
         28
        ],
        [
         "UT",
         "MTN",
         1723,
         492,
         539,
         5,
         2.993,
         25
        ],
        [
         "VT",
         "NE",
         563,
         431,
         466,
         62,
         5.74,
         31
        ],
        [
         "VA",
         "SA",
         6187,
         425,
         470,
         58,
         5.36,
         32
        ],
        [
         "WA",
         "PAC",
         4867,
         437,
         486,
         44,
         5.045,
         33
        ],
        [
         "WV",
         "SA",
         1793,
         443,
         490,
         15,
         5.046,
         26
        ],
        [
         "WI",
         "ENC",
         4892,
         476,
         543,
         11,
         5.946,
         33
        ],
        [
         "WY",
         "MTN",
         454,
         458,
         519,
         13,
         5.255,
         29
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pop",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "SATV",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "SATM",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "percent",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dollars",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pay",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- _c0: string (nullable = true)\n |-- region: string (nullable = true)\n |-- pop: integer (nullable = true)\n |-- SATV: integer (nullable = true)\n |-- SATM: integer (nullable = true)\n |-- percent: integer (nullable = true)\n |-- dollars: double (nullable = true)\n |-- pay: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "                 StructField(\"_c0\", StringType(), True),\n",
    "                 StructField(\"region\", StringType(), True),\n",
    "                 StructField(\"pop\", IntegerType(), True),\n",
    "                 StructField(\"SATV\", IntegerType(), True),\n",
    "                 StructField(\"SATM\", IntegerType(), True),\n",
    "                 StructField(\"percent\", IntegerType(), True),\n",
    "                 StructField(\"dollars\", DoubleType(), True),\n",
    "                 StructField(\"pay\", IntegerType(), True),\n",
    "])\n",
    " \n",
    "filePath = \"dbfs:/databricks-datasets/Rdatasets/data-001/csv/car/States.csv\"\n",
    "df = spark.read.format(\"csv\") \\\n",
    "        .option(\"header\",\"true\") \\\n",
    "        .schema(schema) \\\n",
    "        .load(filePath)\n",
    " \n",
    "display(df)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4b8503f-b227-43eb-a885-e115aab7cf3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Zadanie 3\n",
    "Użycie Read Modes. \n",
    "\n",
    "Wykorzystaj posiadane pliki bądź użyj nowe.  Użyj Sparka do odczytania jednego pliku i użyj wszystkich typów read modes. Poprawny plik nie wywoła żadnych efektów, więc popsuj dane tak aby każda z read modes zadziałał.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d1dd3e2-af10-4584-a462-19e825b825de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-----+----+----+-------+-------+---+\n|_c0|region|  pop|SATV|SATM|percent|dollars|pay|\n+---+------+-----+----+----+-------+-------+---+\n| AL|   ESC| null| 470| 514|      8|  3.648| 27|\n| AK|   PAC|  550|null| 476|     42|  7.887| 43|\n| AZ|   MTN| 3665| 445|null|     25|  4.231| 30|\n| AR|   WSC| 2351| 470| 511|      6|  3.334| 23|\n| CA|   PAC|29760| 419| 484|     45|  4.826| 39|\n| CO|   MTN| 3294| 456| 513|     28|  4.809| 31|\n| CN|    NE| 3287| 430| 471|     74|  7.914| 43|\n| DE|    SA|  666| 433| 470|     58|  6.016| 35|\n| DC|    SA|  607| 409| 441|     68|   8.21| 39|\n| FL|    SA|12938| 418| 466|     44|  5.154| 30|\n| GA|    SA| 6478| 401| 443|     57|   4.86| 29|\n| HI|   PAC| 1108| 404| 481|     52|  5.008| 32|\n| ID|   MTN| 1007| 466| 502|     17|    3.2| 25|\n| IL|   ENC|11431| 466| 528|     16|  5.062| 34|\n| IN|   ENC| 5544| 408| 459|     54|  5.051| 32|\n| IA|   WNC| 2777| 511| 577|      5|  4.839| 28|\n| KS|   WNC| 2478| 492| 548|     10|  5.009| 29|\n| KY|   ESC| 3685| 473| 521|     10|   4.39| 29|\n| LA|   WSC| 4220| 476| 517|      9|  4.012| 26|\n| ME|    NE| 1228| 423| 463|     60|  5.894| 28|\n+---+------+-----+----+----+-------+-------+---+\nonly showing top 20 rows\n\n+---+------+-----+----+----+-------+-------+---+\n|_c0|region|  pop|SATV|SATM|percent|dollars|pay|\n+---+------+-----+----+----+-------+-------+---+\n| AR|   WSC| 2351| 470| 511|      6|  3.334| 23|\n| CA|   PAC|29760| 419| 484|     45|  4.826| 39|\n| CO|   MTN| 3294| 456| 513|     28|  4.809| 31|\n| CN|    NE| 3287| 430| 471|     74|  7.914| 43|\n| DE|    SA|  666| 433| 470|     58|  6.016| 35|\n| DC|    SA|  607| 409| 441|     68|   8.21| 39|\n| FL|    SA|12938| 418| 466|     44|  5.154| 30|\n| GA|    SA| 6478| 401| 443|     57|   4.86| 29|\n| HI|   PAC| 1108| 404| 481|     52|  5.008| 32|\n| ID|   MTN| 1007| 466| 502|     17|    3.2| 25|\n| IL|   ENC|11431| 466| 528|     16|  5.062| 34|\n| IN|   ENC| 5544| 408| 459|     54|  5.051| 32|\n| IA|   WNC| 2777| 511| 577|      5|  4.839| 28|\n| KS|   WNC| 2478| 492| 548|     10|  5.009| 29|\n| KY|   ESC| 3685| 473| 521|     10|   4.39| 29|\n| LA|   WSC| 4220| 476| 517|      9|  4.012| 26|\n| ME|    NE| 1228| 423| 463|     60|  5.894| 28|\n| MD|    SA| 4781| 430| 478|     59|  6.184| 38|\n| MA|    NE| 6016| 427| 473|     72|  6.351| 36|\n| MI|   ENC| 9295| 454| 514|     12|  5.257| 38|\n+---+------+-----+----+----+-------+-------+---+\nonly showing top 20 rows\n\nUnexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-3339482602787830>\", line 39, in <module>\n    df_failfast.show()\n  File \"/databricks/spark/python/pyspark/instrumentation_utils.py\", line 48, in wrapper\n    res = func(*args, **kwargs)\n  File \"/databricks/spark/python/pyspark/sql/dataframe.py\", line 920, in show\n    print(self._jdf.showString(n, 20, vertical))\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 228, in deco\n    return f(*a, **kw)\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/protocol.py\", line 326, in get_return_value\n    raise Py4JJavaError(\npy4j.protocol.Py4JJavaError: An error occurred while calling o1101.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 60) (ip-10-172-221-215.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/tmp/States_corrupt.csv/part-00000-tid-6042965612680173238-4034f239-3ee4-4355-9a04-680ada20178b-57-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"INVALID\"\nCaused by: java.lang.NumberFormatException: For input string: \"INVALID\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/tmp/States_corrupt.csv/part-00000-tid-6042965612680173238-4034f239-3ee4-4355-9a04-680ada20178b-57-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"INVALID\"\nCaused by: java.lang.NumberFormatException: For input string: \"INVALID\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>Py4JJavaError</span>: An error occurred while calling o1101.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 34.0 failed 1 times, most recent failure: Lost task 0.0 in stage 34.0 (TID 60) (ip-10-172-221-215.us-west-2.compute.internal executor driver): com.databricks.sql.io.FileReadException: Error while reading file dbfs:/tmp/States_corrupt.csv/part-00000-tid-6042965612680173238-4034f239-3ee4-4355-9a04-680ada20178b-57-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"INVALID\"\nCaused by: java.lang.NumberFormatException: For input string: \"INVALID\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:3440)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:3362)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:3351)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:3351)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1460)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1460)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3651)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3589)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:3577)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:51)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$runJob$1(DAGScheduler.scala:1209)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:1197)\n\tat org.apache.spark.SparkContext.runJobInternal(SparkContext.scala:2758)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$runSparkJobs$1(Collector.scala:349)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.collect.Collector.runSparkJobs(Collector.scala:293)\n\tat org.apache.spark.sql.execution.collect.Collector.collect(Collector.scala:377)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:128)\n\tat org.apache.spark.sql.execution.collect.Collector$.collect(Collector.scala:135)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:122)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:110)\n\tat org.apache.spark.sql.execution.qrc.InternalRowFormat$.collect(cachedSparkResults.scala:92)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$computeResult$1(ResultCacheManager.scala:537)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.collectResult$1(ResultCacheManager.scala:529)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.computeResult(ResultCacheManager.scala:549)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.$anonfun$getOrComputeResultInternal$1(ResultCacheManager.scala:402)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResultInternal(ResultCacheManager.scala:395)\n\tat org.apache.spark.sql.execution.qrc.ResultCacheManager.getOrComputeResult(ResultCacheManager.scala:289)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectResult$1(SparkPlan.scala:506)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:80)\n\tat org.apache.spark.sql.execution.SparkPlan.executeCollectResult(SparkPlan.scala:503)\n\tat org.apache.spark.sql.Dataset.collectResult(Dataset.scala:3458)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4382)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$3(Dataset.scala:4373)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:841)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4371)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$9(SQLExecution.scala:258)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:448)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withCustomExecutionEnv$1(SQLExecution.scala:203)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:1073)\n\tat org.apache.spark.sql.execution.SQLExecution$.withCustomExecutionEnv(SQLExecution.scala:131)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:398)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4371)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3168)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3389)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:315)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:354)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:380)\n\tat py4j.Gateway.invoke(Gateway.java:306)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:195)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:115)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: com.databricks.sql.io.FileReadException: Error while reading file dbfs:/tmp/States_corrupt.csv/part-00000-tid-6042965612680173238-4034f239-3ee4-4355-9a04-680ada20178b-57-1-c000.csv.\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.logFileNameAndThrow(FileScanRDD.scala:704)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:673)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: Malformed records are detected in record parsing. Parse Mode: FAILFAST. To process malformed records as null result, try setting the option 'mode' as 'PERMISSIVE'.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.malformedRecordsDetectedInRecordParsingError(QueryExecutionErrors.scala:1936)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:103)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\t... 32 more\nCaused by: org.apache.spark.sql.catalyst.util.BadRecordException: java.lang.NumberFormatException: For input string: \"INVALID\"\nCaused by: java.lang.NumberFormatException: For input string: \"INVALID\"\n\tat java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)\n\tat java.lang.Integer.parseInt(Integer.java:580)\n\tat java.lang.Integer.parseInt(Integer.java:615)\n\tat scala.collection.immutable.StringLike.toInt(StringLike.scala:304)\n\tat scala.collection.immutable.StringLike.toInt$(StringLike.scala:304)\n\tat scala.collection.immutable.StringOps.toInt(StringOps.scala:33)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$6$adapted(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.nullSafeDatum(UnivocityParser.scala:305)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$makeConverter$5(UnivocityParser.scala:202)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.convert(UnivocityParser.scala:360)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser.$anonfun$parse$2(UnivocityParser.scala:321)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$1(UnivocityParser.scala:499)\n\tat org.apache.spark.sql.catalyst.util.FailureSafeParser.parse(FailureSafeParser.scala:92)\n\tat org.apache.spark.sql.catalyst.csv.UnivocityParser$.$anonfun$parseIterator$2(UnivocityParser.scala:507)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.util.CompletionIterator.hasNext(CompletionIterator.scala:31)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1$$anon$2.getNext(FileScanRDD.scala:619)\n\tat org.apache.spark.util.NextIterator.hasNext(NextIterator.scala:73)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:796)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.$anonfun$hasNext$1(FileScanRDD.scala:496)\n\tat scala.runtime.java8.JFunction0$mcZ$sp.apply(JFunction0$mcZ$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:486)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:761)\n\tat org.apache.spark.sql.execution.collect.UnsafeRowBatchUtils$.encodeUnsafeRows(UnsafeRowBatchUtils.scala:82)\n\tat org.apache.spark.sql.execution.collect.Collector.$anonfun$processFunc$1(Collector.scala:208)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$3(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.$anonfun$runTask$1(ResultTask.scala:75)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:55)\n\tat org.apache.spark.scheduler.Task.doRunTask(Task.scala:179)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$5(Task.scala:142)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:126)\n\tat org.apache.spark.scheduler.Task.$anonfun$run$1(Task.scala:142)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:97)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$13(Executor.scala:904)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1741)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:907)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.spark.util.ExecutorFrameProfiler$.record(ExecutorFrameProfiler.scala:110)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:761)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:750)\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Wprowadzenie błędnych danych do niektórych wierszy\n",
    "corrupt_df = df.withColumn(\"pop\", when(df[\"_c0\"] == \"AL\", \"INVALID\").otherwise(df[\"pop\"]))\\\n",
    "             .withColumn(\"SATV\", when(df[\"_c0\"] == \"AK\", \"INVALID\").otherwise(df[\"SATV\"]))\\\n",
    "             .withColumn(\"SATM\", when(df[\"_c0\"] == \"AZ\", \"INVALID\").otherwise(df[\"SATM\"]))\n",
    "\n",
    "# Zapisanie do nowego pliku CSV\n",
    "filePath_corrupt = \"dbfs:/tmp/States_corrupt.csv\"\n",
    "corrupt_df.write.mode(\"overwrite\").option(\"header\", \"true\").csv(filePath_corrupt)\n",
    "\n",
    "\n",
    "# Tryb permissive (Błędne wartości zostają zastąpione NULL)\n",
    "df_permissive = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(filePath_corrupt)\n",
    "\n",
    "df_permissive.show()\n",
    "\n",
    "\n",
    "# Tryb DROPMALFORMED (Wiersze z błędami są usuwane)\n",
    "df_dropmalformed = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(filePath_corrupt)\n",
    "\n",
    "df_dropmalformed.show()\n",
    "\n",
    "# Tryb FAILFAST (Jeśli znajdzie błąd, zatrzyma działanie)\n",
    "df_failfast = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"mode\", \"FAILFAST\") \\\n",
    "    .schema(schema) \\\n",
    "    .load(filePath_corrupt)\n",
    "\n",
    "df_failfast.show() \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e79cc6-0fe5-4f81-a310-b2e2151e54e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Zadanie 4\n",
    "Użycie DataFrameWriter.\n",
    "\n",
    "Zapisz jeden z wybranych plików do formatów (‘.parquet’, ‘.json’). Sprawdź, czy dane są zapisane poprawnie, użyj do tego spark.read (DataFrameReader). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbe99120-57f0-4338-8beb-8439ec491974",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>region</th><th>pop</th><th>SATV</th><th>SATM</th><th>percent</th><th>dollars</th><th>pay</th></tr></thead><tbody><tr><td>AL</td><td>ESC</td><td>INVALID</td><td>470</td><td>514</td><td>8</td><td>3.648</td><td>27</td></tr><tr><td>AK</td><td>PAC</td><td>550</td><td>INVALID</td><td>476</td><td>42</td><td>7.887</td><td>43</td></tr><tr><td>AZ</td><td>MTN</td><td>3665</td><td>445</td><td>INVALID</td><td>25</td><td>4.231</td><td>30</td></tr><tr><td>AR</td><td>WSC</td><td>2351</td><td>470</td><td>511</td><td>6</td><td>3.334</td><td>23</td></tr><tr><td>CA</td><td>PAC</td><td>29760</td><td>419</td><td>484</td><td>45</td><td>4.826</td><td>39</td></tr><tr><td>CO</td><td>MTN</td><td>3294</td><td>456</td><td>513</td><td>28</td><td>4.809</td><td>31</td></tr><tr><td>CN</td><td>NE</td><td>3287</td><td>430</td><td>471</td><td>74</td><td>7.914</td><td>43</td></tr><tr><td>DE</td><td>SA</td><td>666</td><td>433</td><td>470</td><td>58</td><td>6.016</td><td>35</td></tr><tr><td>DC</td><td>SA</td><td>607</td><td>409</td><td>441</td><td>68</td><td>8.21</td><td>39</td></tr><tr><td>FL</td><td>SA</td><td>12938</td><td>418</td><td>466</td><td>44</td><td>5.154</td><td>30</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "AL",
         "ESC",
         "INVALID",
         "470",
         "514",
         8,
         3.648,
         27
        ],
        [
         "AK",
         "PAC",
         "550",
         "INVALID",
         "476",
         42,
         7.887,
         43
        ],
        [
         "AZ",
         "MTN",
         "3665",
         "445",
         "INVALID",
         25,
         4.231,
         30
        ],
        [
         "AR",
         "WSC",
         "2351",
         "470",
         "511",
         6,
         3.334,
         23
        ],
        [
         "CA",
         "PAC",
         "29760",
         "419",
         "484",
         45,
         4.826,
         39
        ],
        [
         "CO",
         "MTN",
         "3294",
         "456",
         "513",
         28,
         4.809,
         31
        ],
        [
         "CN",
         "NE",
         "3287",
         "430",
         "471",
         74,
         7.914,
         43
        ],
        [
         "DE",
         "SA",
         "666",
         "433",
         "470",
         58,
         6.016,
         35
        ],
        [
         "DC",
         "SA",
         "607",
         "409",
         "441",
         68,
         8.21,
         39
        ],
        [
         "FL",
         "SA",
         "12938",
         "418",
         "466",
         44,
         5.154,
         30
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "pop",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SATV",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SATM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "percent",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "dollars",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pay",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>SATM</th><th>SATV</th><th>_c0</th><th>dollars</th><th>pay</th><th>percent</th><th>pop</th><th>region</th></tr></thead><tbody><tr><td>514</td><td>470</td><td>AL</td><td>3.648</td><td>27</td><td>8</td><td>INVALID</td><td>ESC</td></tr><tr><td>476</td><td>INVALID</td><td>AK</td><td>7.887</td><td>43</td><td>42</td><td>550</td><td>PAC</td></tr><tr><td>INVALID</td><td>445</td><td>AZ</td><td>4.231</td><td>30</td><td>25</td><td>3665</td><td>MTN</td></tr><tr><td>511</td><td>470</td><td>AR</td><td>3.334</td><td>23</td><td>6</td><td>2351</td><td>WSC</td></tr><tr><td>484</td><td>419</td><td>CA</td><td>4.826</td><td>39</td><td>45</td><td>29760</td><td>PAC</td></tr><tr><td>513</td><td>456</td><td>CO</td><td>4.809</td><td>31</td><td>28</td><td>3294</td><td>MTN</td></tr><tr><td>471</td><td>430</td><td>CN</td><td>7.914</td><td>43</td><td>74</td><td>3287</td><td>NE</td></tr><tr><td>470</td><td>433</td><td>DE</td><td>6.016</td><td>35</td><td>58</td><td>666</td><td>SA</td></tr><tr><td>441</td><td>409</td><td>DC</td><td>8.21</td><td>39</td><td>68</td><td>607</td><td>SA</td></tr><tr><td>466</td><td>418</td><td>FL</td><td>5.154</td><td>30</td><td>44</td><td>12938</td><td>SA</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "514",
         "470",
         "AL",
         3.648,
         27,
         8,
         "INVALID",
         "ESC"
        ],
        [
         "476",
         "INVALID",
         "AK",
         7.887,
         43,
         42,
         "550",
         "PAC"
        ],
        [
         "INVALID",
         "445",
         "AZ",
         4.231,
         30,
         25,
         "3665",
         "MTN"
        ],
        [
         "511",
         "470",
         "AR",
         3.334,
         23,
         6,
         "2351",
         "WSC"
        ],
        [
         "484",
         "419",
         "CA",
         4.826,
         39,
         45,
         "29760",
         "PAC"
        ],
        [
         "513",
         "456",
         "CO",
         4.809,
         31,
         28,
         "3294",
         "MTN"
        ],
        [
         "471",
         "430",
         "CN",
         7.914,
         43,
         74,
         "3287",
         "NE"
        ],
        [
         "470",
         "433",
         "DE",
         6.016,
         35,
         58,
         "666",
         "SA"
        ],
        [
         "441",
         "409",
         "DC",
         8.21,
         39,
         68,
         "607",
         "SA"
        ],
        [
         "466",
         "418",
         "FL",
         5.154,
         30,
         44,
         "12938",
         "SA"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "SATM",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "SATV",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "dollars",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "pay",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "percent",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "pop",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Zapisanie danych do formatu .parquet\n",
    "parquet_path = \"dbfs:/FileStore/tables/States_corrupt.parquet\"\n",
    "corrupt_df.write.format(\"parquet\").mode(\"overwrite\").option(\"header\", \"true\").save(parquet_path)\n",
    "\n",
    "parquet_df = spark.read.format(\"parquet\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(parquet_path)\n",
    "    \n",
    "display(parquet_df.limit(10))\n",
    "\n",
    "\n",
    "# Zapisanie danych do formatu .json\n",
    "json_path = \"dbfs:/FileStore/tables/States_corrupt.json\"\n",
    "corrupt_df.write.format(\"json\").mode(\"overwrite\").option(\"header\", \"true\").save(json_path)\n",
    "\n",
    "json_df = spark.read.format(\"json\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .load(json_path)\n",
    "    \n",
    "display(json_df.limit(10))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4474089525096717,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "lab2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
